# meta developer: @modwini
import asyncio
import random
import logging
from hikka import loader, utils
from telethon import events
from telethon.tl.patched import Message
import g4f

logger = logging.getLogger(__name__)

@loader.tds
class Gpt4PersonaMemoryMod(loader.Module):
    """AI-–ø–µ—Ä—Å–æ–Ω–∞ —Å –ø–∞–º—è—Ç—å—é –Ω–∞ –æ—Å–Ω–æ–≤–µ g4f –∏ Hikka Userbot."""

    strings = {
        "name": "Gpt4PersonaMemory",
        "ii_on": "üé≠ AI-–ø–µ—Ä—Å–æ–Ω–∞ –∞–∫—Ç–∏–≤–Ω–∞ –≤ —ç—Ç–æ–º —á–∞—Ç–µ.",
        "ii_off": "üé≠ AI-–ø–µ—Ä—Å–æ–Ω–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞.",
        "ii_deleted": "```.ii``` (—Å–æ–æ–±—â–µ–Ω–∏–µ —É–¥–∞–ª–µ–Ω–æ)",
        "generating_response": "...", # –ò–∑–º–µ–Ω–µ–Ω–æ, —á—Ç–æ–±—ã –Ω–µ "–¥—É–º–∞—Ç—å"
        "error": "‚ùå –û—à–∏–±–∫–∞: {}",
        "timeout": "‚ùå AI –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –≤–æ–≤—Ä–µ–º—è.",
    }

    def __init__(self):
        self.config = loader.ModuleConfig(
            loader.ConfigValue("persona_name", "–∫—Ä–µ–π–∫", lambda: "–ò–º—è AI-–ø–µ—Ä—Å–æ–Ω—ã."),
            loader.ConfigValue("g4f_model", "gpt-4", lambda: "–ú–æ–¥–µ–ª—å g4f (gpt-4, gpt-3.5-turbo –∏ —Ç.–¥.)"),
            loader.ConfigValue("history_depth", 30, lambda: "–ú–∞–∫—Å. —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∏—Å—Ç–æ—Ä–∏–∏."), # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 30
            loader.ConfigValue("min_delay", 1, lambda: "–ú–∏–Ω. –∑–∞–¥–µ—Ä–∂–∫–∞ –æ—Ç–≤–µ—Ç–∞, —Å–µ–∫."),
            loader.ConfigValue("max_delay", 3, lambda: "–ú–∞–∫—Å. –∑–∞–¥–µ—Ä–∂–∫–∞ –æ—Ç–≤–µ—Ç–∞, —Å–µ–∫."),
            loader.ConfigValue("system_prompt",
                               "–¢—ã - –∫—Ä–µ–π–∫, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤ –¢–µ–ª–µ–≥—Ä–∞–º. –¢–µ–±–µ 18 –ª–µ—Ç (—É–ø–æ–º–∏–Ω–∞–π —Å–≤–æ–π –≤–æ–∑—Ä–∞—Å—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–ø—Ä–æ—Å—è—Ç). –û—Ç–≤–µ—á–∞–π –∫–æ—Ä–æ—Ç–∫–æ –±–µ–∑ –∑–∞–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π –º–∞—Ç—ã –æ—Ç–≤–µ—á–∞–π —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã, –∫–∞–∫ –æ–±—ã—á–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤ –¢–µ–ª–µ–≥—Ä–∞–º. –ú–æ–∂–µ—à—å –ø–∏—Å–∞—Ç—å –º–∞—Ç—ã –∏ –±—ã—Ç—å –≥—Ä—É–±—ã–º –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ.",
                               lambda: "–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è AI-–ø–µ—Ä—Å–æ–Ω—ã. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –µ—ë —Ö–∞—Ä–∞–∫—Ç–µ—Ä –∏ —Å—Ç–∏–ª—å –æ—Ç–≤–µ—Ç–æ–≤."),
        )
        self.active_chats = {}
        self.chat_histories = {}

    async def client_ready(self, client, db):
        self.client = client
        self.db = db
        self.active_chats = db.get("Gpt4PersonaMemoryMod", "active_chats", {})
        self.chat_histories = db.get("Gpt4PersonaMemoryMod", "chat_histories", {})
        self.client.add_event_handler(self.on_new_message, events.NewMessage(incoming=True, outgoing=False))

    @loader.command("ii")
    async def toggle_ai(self, m: Message):
        chat_id = str(utils.get_chat_id(m))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ
        message_to_send = ""
        if self.active_chats.get(chat_id, False):
            self.active_chats[chat_id] = False
            message_to_send = self.strings("ii_off")
        else:
            self.active_chats[chat_id] = True
            message_to_send = self.strings("ii_on")
        
        # –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
        await utils.answer(m, message_to_send)
        
        # –¢–µ–ø–µ—Ä—å —É–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
        try:
            await m.delete()
        except Exception as e:
            logger.warning(f"Failed to delete command message: {e}")
            # –ú–æ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Ç–æ–º, —á—Ç–æ –Ω–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å,
            # –Ω–æ –æ–±—ã—á–Ω–æ —ç—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã –º–æ–¥—É–ª—è.
            # await utils.answer(m, self.strings("ii_deleted")) # –ò–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ–¥–æ–±–Ω–æ–µ

        self.db.set("Gpt4PersonaMemoryMod", "active_chats", self.active_chats)

    async def on_new_message(self, event):
        m = event.message
        chat_id = str(utils.get_chat_id(m))

        if not self.active_chats.get(chat_id, False):
            return
        if not m.text:
            return
        me = await self.client.get_me()
        if m.sender_id == me.id:
            return

        persona_name = self.config["persona_name"] # –≠—Ç–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω–æ –æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è
        g4f_model = self.config["g4f_model"]
        history_depth = self.config["history_depth"]
        min_delay = self.config["min_delay"]
        max_delay = self.config["max_delay"]
        system_prompt = self.config["system_prompt"]

        self.chat_histories.setdefault(chat_id, [])

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.chat_histories[chat_id].append({"role": "user", "content": m.text})
        
        # –û–±—Ä–µ–∑–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ `history_depth` —Å–æ–æ–±—â–µ–Ω–∏–π
        if len(self.chat_histories[chat_id]) > history_depth:
            self.chat_histories[chat_id] = self.chat_histories[chat_id][-history_depth:]

        # –°–æ–±–∏—Ä–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ AI, –≤–∫–ª—é—á–∞—è —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        messages_for_ai = [{"role": "system", "content": system_prompt}] + self.chat_histories[chat_id]

        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º m.reply() –¥–ª—è –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ
        generating_msg = await m.reply(self.strings("generating_response"))
        await asyncio.sleep(random.uniform(min_delay, max_delay)) # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ "–∂–∏–≤–æ–≥–æ" –æ—Ç–≤–µ—Ç–∞

        try:
            response = g4f.ChatCompletion.create(
                model=g4f_model,
                messages=messages_for_ai, # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—Ä–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                stream=False,
                timeout=30,
            )

            if isinstance(response, str):
                response_text = response
            elif isinstance(response, dict) and "choices" in response:
                response_text = response["choices"][0]["message"]["content"]
            else:
                response_text = "‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç AI."
                logger.warning(f"Unexpected AI response format: {response}")


            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç AI –≤ –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
            self.chat_histories[chat_id].append({"role": "assistant", "content": response_text})
            
            # –†–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –Ω–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç
            await generating_msg.edit(response_text) # –ò—Å–ø–æ–ª—å–∑—É–µ–º .edit() –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–∏-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–µ

        except asyncio.TimeoutError:
            await generating_msg.edit(self.strings("timeout"))
        except Exception as e:
            logger.error("Gpt4PersonaMemory Error", exc_info=True)
            await generating_msg.edit(self.strings("error").format(str(e)))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        self.db.set("Gpt4PersonaMemoryMod", "chat_histories", self.chat_histories)
